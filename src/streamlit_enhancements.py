# src/streamlit_enhancements.py
"""
Enhanced Streamlit Components for Legal Research Engine

This module provides advanced UI components, dashboards, and interactive
elements to enhance the user experience with modern Streamlit features.
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import json

from .langgraph_integration import LangGraphRAGIntegration
from .langgraph_workflow import get_legal_workflow
from .monitoring import get_monitor

def create_modern_header():
    """Create an enhanced modern header with metrics and status"""
    
    # Custom CSS for modern styling
    st.markdown("""
    <style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        color: white;
        margin-bottom: 2rem;
        box-shadow: 0 8px 32px rgba(31, 38, 135, 0.37);
        backdrop-filter: blur(4px);
        border: 1px solid rgba(255, 255, 255, 0.18);
    }
    
    .status-badge {
        display: inline-block;
        padding: 0.3rem 0.8rem;
        border-radius: 20px;
        font-size: 0.8rem;
        font-weight: bold;
        margin: 0.2rem;
    }
    
    .status-online {
        background-color: #10b981;
        color: white;
    }
    
    .status-offline {
        background-color: #ef4444;
        color: white;
    }
    
    .metric-card {
        background: rgba(255, 255, 255, 0.1);
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem;
        text-align: center;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Check system status
    lm_studio_status = check_lm_studio_status()
    workflow_status = check_workflow_status()
    
    st.markdown(f"""
    <div class="main-header">
        <h1>üèõÔ∏è Legal Research Engine with LangGraph</h1>
        <p>Advanced AI-powered legal research using multi-agent workflows and local LM Studio models</p>
        <div style="margin-top: 1rem;">
            <span class="status-badge {'status-online' if lm_studio_status else 'status-offline'}">
                {'üü¢ LM Studio Online' if lm_studio_status else 'üî¥ LM Studio Offline'}
            </span>
            <span class="status-badge {'status-online' if workflow_status else 'status-offline'}">
                {'üü¢ LangGraph Ready' if workflow_status else 'üî¥ LangGraph Error'}
            </span>
        </div>
    </div>
    """, unsafe_allow_html=True)


def create_analytics_dashboard():
    """Create a comprehensive analytics dashboard"""
    
    st.subheader("üìä System Analytics Dashboard")
    
    # Generate sample analytics data
    analytics_data = generate_analytics_data()
    
    # Create tabs for different analytics views
    tab1, tab2, tab3, tab4 = st.tabs([
        "üìà Usage Metrics", 
        "ü§ñ Agent Performance", 
        "‚ö° System Health", 
        "üìä Query Analysis"
    ])
    
    with tab1:
        create_usage_metrics(analytics_data)
    
    with tab2:
        create_agent_performance_dashboard(analytics_data)
    
    with tab3:
        create_system_health_dashboard()
    
    with tab4:
        create_query_analysis_dashboard()


def create_usage_metrics(data: Dict[str, Any]):
    """Create usage metrics visualization"""
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "Total Queries Today",
            data['daily_queries'],
            delta=f"+{data['query_growth']}%",
            help="Number of queries processed today vs yesterday"
        )
    
    with col2:
        st.metric(
            "Active Users",
            data['active_users'],
            delta=f"+{data['user_growth']}",
            help="Currently active users in the system"
        )
    
    with col3:
        st.metric(
            "Avg Response Time",
            f"{data['avg_response_time']}s",
            delta=f"-{data['response_improvement']}s",
            delta_color="inverse",
            help="Average time to generate responses"
        )
    
    with col4:
        st.metric(
            "Success Rate",
            f"{data['success_rate']}%",
            delta=f"+{data['success_improvement']}%",
            help="Percentage of successful query completions"
        )
    
    # Usage over time chart
    st.subheader("üìà Usage Trends")
    
    # Generate time series data
    dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')
    usage_data = pd.DataFrame({
        'Date': dates,
        'Traditional_RAG': np.random.poisson(50, len(dates)) + np.sin(np.arange(len(dates)) / 30) * 10,
        'LangGraph_Workflow': np.random.poisson(30, len(dates)) + np.sin(np.arange(len(dates)) / 30) * 15,
        'API_Calls': np.random.poisson(25, len(dates)) + np.cos(np.arange(len(dates)) / 20) * 8
    })
    
    fig = px.line(
        usage_data.melt(id_vars=['Date'], var_name='Method', value_name='Count'),
        x='Date', 
        y='Count', 
        color='Method',
        title='Query Usage Over Time',
        labels={'Count': 'Number of Queries', 'Method': 'Query Method'}
    )
    
    fig.update_layout(
        hovermode='x unified',
        showlegend=True,
        height=400
    )
    
    st.plotly_chart(fig, use_container_width=True)


def create_agent_performance_dashboard(data: Dict[str, Any]):
    """Create agent performance visualization"""
    
    # Agent performance metrics
    agent_data = pd.DataFrame({
        'Agent': [
            'Document Retrieval',
            'Case Brief Generator', 
            'Precedent Analyzer',
            'Citation Extractor',
            'Report Synthesizer'
        ],
        'Success_Rate': [95, 88, 92, 85, 96],
        'Avg_Time': [2.3, 8.5, 12.1, 5.2, 6.8],
        'Queries_Handled': [1250, 890, 756, 923, 845]
    })
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Agent success rates
        fig1 = px.bar(
            agent_data,
            x='Agent',
            y='Success_Rate',
            title='Agent Success Rates',
            color='Success_Rate',
            color_continuous_scale='Viridis'
        )
        fig1.update_layout(xaxis_tickangle=-45, height=400)
        st.plotly_chart(fig1, use_container_width=True)
    
    with col2:
        # Agent response times
        fig2 = px.scatter(
            agent_data,
            x='Avg_Time',
            y='Queries_Handled',
            size='Success_Rate',
            color='Agent',
            title='Agent Performance Matrix',
            labels={
                'Avg_Time': 'Average Response Time (s)',
                'Queries_Handled': 'Queries Handled'
            }
        )
        fig2.update_layout(height=400)
        st.plotly_chart(fig2, use_container_width=True)
    
    # Agent efficiency radar chart
    st.subheader("üéØ Agent Efficiency Radar")
    
    categories = agent_data['Agent'].tolist()
    
    fig3 = go.Figure()
    
    fig3.add_trace(go.Scatterpolar(
        r=agent_data['Success_Rate'].tolist() + [agent_data['Success_Rate'].iloc[0]],
        theta=categories + [categories[0]],
        fill='toself',
        name='Success Rate'
    ))
    
    fig3.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100]
            )),
        showlegend=True,
        height=500
    )
    
    st.plotly_chart(fig3, use_container_width=True)


def create_system_health_dashboard():
    """Create system health monitoring dashboard"""
    
    st.subheader("‚ö° Real-time System Health")
    
    # Create columns for different health metrics
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # CPU Usage gauge
        cpu_usage = np.random.uniform(20, 80)
        fig_cpu = go.Figure(go.Indicator(
            mode="gauge+number+delta",
            value=cpu_usage,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': "CPU Usage (%)"},
            delta={'reference': 50},
            gauge={
                'axis': {'range': [None, 100]},
                'bar': {'color': "darkblue"},
                'steps': [
                    {'range': [0, 50], 'color': "lightgray"},
                    {'range': [50, 80], 'color': "gray"}],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75, 'value': 90}}))
        fig_cpu.update_layout(height=300)
        st.plotly_chart(fig_cpu, use_container_width=True)
    
    with col2:
        # Memory Usage gauge
        memory_usage = np.random.uniform(30, 90)
        fig_mem = go.Figure(go.Indicator(
            mode="gauge+number+delta",
            value=memory_usage,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': "Memory Usage (%)"},
            delta={'reference': 60},
            gauge={
                'axis': {'range': [None, 100]},
                'bar': {'color': "darkgreen"},
                'steps': [
                    {'range': [0, 60], 'color': "lightgray"},
                    {'range': [60, 85], 'color': "gray"}],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75, 'value': 95}}))
        fig_mem.update_layout(height=300)
        st.plotly_chart(fig_mem, use_container_width=True)
    
    with col3:
        # Response Time gauge
        response_time = np.random.uniform(5, 25)
        fig_resp = go.Figure(go.Indicator(
            mode="gauge+number+delta",
            value=response_time,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': "Avg Response Time (s)"},
            delta={'reference': 15},
            gauge={
                'axis': {'range': [0, 30]},
                'bar': {'color': "darkorange"},
                'steps': [
                    {'range': [0, 10], 'color': "lightgray"},
                    {'range': [10, 20], 'color': "gray"}],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75, 'value': 25}}))
        fig_resp.update_layout(height=300)
        st.plotly_chart(fig_resp, use_container_width=True)
    
    # System logs
    st.subheader("üìã Recent System Events")
    
    # Create sample log data
    log_data = pd.DataFrame({
        'Timestamp': [
            datetime.now() - timedelta(minutes=i*5) for i in range(10)
        ],
        'Level': np.random.choice(['INFO', 'WARNING', 'ERROR'], 10, p=[0.7, 0.2, 0.1]),
        'Component': np.random.choice(['LangGraph', 'LM Studio', 'Vector Store', 'API'], 10),
        'Message': [
            'Query processed successfully',
            'Model loaded',
            'Cache updated',
            'New document indexed',
            'User session started',
            'Workflow completed',
            'Agent initialized',
            'Memory threshold reached',
            'Connection established',
            'Background task completed'
        ]
    })
    
    # Color code by log level
    def color_log_level(val):
        color_map = {
            'INFO': 'background-color: #d4edda',
            'WARNING': 'background-color: #fff3cd', 
            'ERROR': 'background-color: #f8d7da'
        }
        return [color_map.get(v, '') for v in val]
    
    styled_logs = log_data.style.apply(color_log_level, subset=['Level'])
    st.dataframe(styled_logs, use_container_width=True, height=300)


def create_query_analysis_dashboard():
    """Create query analysis and insights dashboard"""
    
    st.subheader("üîç Query Analysis & Insights")
    
    # Query categories analysis
    col1, col2 = st.columns(2)
    
    with col1:
        # Query types pie chart
        query_types = {
            'Commercial Law': 35,
            'Civil Procedure': 28,
            'Constitutional Law': 20,
            'Criminal Law': 12,
            'Other': 5
        }
        
        fig_pie = px.pie(
            values=list(query_types.values()),
            names=list(query_types.keys()),
            title='Query Categories Distribution'
        )
        st.plotly_chart(fig_pie, use_container_width=True)
    
    with col2:
        # Query complexity distribution
        complexity_data = pd.DataFrame({
            'Complexity': ['Simple', 'Moderate', 'Complex', 'Advanced'],
            'Count': [150, 120, 80, 25],
            'Avg_Time': [3, 8, 15, 25]
        })
        
        fig_complexity = px.bar(
            complexity_data,
            x='Complexity',
            y='Count',
            color='Avg_Time',
            title='Query Complexity Distribution',
            color_continuous_scale='Reds'
        )
        st.plotly_chart(fig_complexity, use_container_width=True)
    
    # Most common search terms
    st.subheader("üî• Trending Legal Topics")
    
    trending_topics = pd.DataFrame({
        'Topic': [
            'Commercial Courts Act',
            'Section 138 NI Act', 
            'Article 226 Constitution',
            'Summary Judgment',
            'Arbitration Agreement',
            'Specific Performance',
            'Limitation Period',
            'Interim Orders'
        ],
        'Frequency': [45, 38, 32, 28, 25, 22, 20, 18],
        'Growth': ['+15%', '+8%', '+22%', '+5%', '+12%', '-3%', '+7%', '+18%']
    })
    
    # Create horizontal bar chart
    fig_trending = px.bar(
        trending_topics,
        x='Frequency',
        y='Topic',
        orientation='h',
        title='Most Searched Legal Topics',
        color='Frequency',
        color_continuous_scale='Blues'
    )
    fig_trending.update_layout(height=400)
    st.plotly_chart(fig_trending, use_container_width=True)


def create_interactive_workflow_builder():
    """Create an interactive workflow builder interface"""
    
    st.subheader("üîß Interactive Workflow Builder")
    st.write("Design and customize your legal research workflow")
    
    # Workflow components
    available_agents = {
        'Document Retrieval': {
            'description': 'Retrieves relevant legal documents',
            'time': '2-5s',
            'required': True
        },
        'Case Brief Generator': {
            'description': 'Creates structured case summaries',
            'time': '8-15s',
            'required': False
        },
        'Precedent Analyzer': {
            'description': 'Finds similar cases and legal principles', 
            'time': '10-20s',
            'required': False
        },
        'Citation Extractor': {
            'description': 'Extracts legal citations automatically',
            'time': '3-8s',
            'required': False
        },
        'Report Synthesizer': {
            'description': 'Combines all analysis into final report',
            'time': '5-10s',
            'required': True
        }
    }
    
    st.info("üí° Select the agents you want to include in your custom workflow")
    
    selected_agents = []
    estimated_time = 0
    
    for agent, config in available_agents.items():
        col1, col2 = st.columns([3, 1])
        
        with col1:
            if config['required']:
                selected = st.checkbox(
                    f"**{agent}** *(Required)*", 
                    value=True, 
                    disabled=True,
                    help=config['description']
                )
            else:
                selected = st.checkbox(
                    f"**{agent}**",
                    value=False,
                    help=config['description']
                )
        
        with col2:
            if selected or config['required']:
                st.caption(f"‚è±Ô∏è {config['time']}")
                estimated_time += float(config['time'].split('-')[1][:-1])  # Get max time
                selected_agents.append(agent)
    
    # Display workflow summary
    if selected_agents:
        st.subheader("üìã Workflow Summary")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Selected Agents", len(selected_agents))
        with col2:
            st.metric("Estimated Time", f"{estimated_time:.0f}s")
        with col3:
            quality_score = min(100, 40 + len(selected_agents) * 15)
            st.metric("Quality Score", f"{quality_score}%")
        
        # Workflow visualization
        st.subheader("üîÑ Workflow Flow")
        
        # Create a simple workflow diagram using Mermaid
        mermaid_code = "flowchart TD\n    Start([Start Query])\n"
        
        for i, agent in enumerate(selected_agents):
            agent_id = agent.replace(' ', '_').replace('-', '_')
            mermaid_code += f"    {agent_id}[{agent}]\n"
            
            if i == 0:
                mermaid_code += f"    Start --> {agent_id}\n"
            else:
                prev_agent = selected_agents[i-1].replace(' ', '_').replace('-', '_')
                mermaid_code += f"    {prev_agent} --> {agent_id}\n"
        
        last_agent = selected_agents[-1].replace(' ', '_').replace('-', '_')
        mermaid_code += f"    {last_agent} --> End([Complete])\n"
        
        st.code(mermaid_code, language='mermaid')
        
        # Save/Load workflow buttons
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("üíæ Save Workflow"):
                # Save to session state
                st.session_state.custom_workflow = selected_agents
                st.success("Workflow saved!")
        
        with col2:
            if st.button("üîÑ Load Default"):
                st.session_state.custom_workflow = list(available_agents.keys())
                st.success("Default workflow loaded!")
        
        with col3:
            if st.button("‚ñ∂Ô∏è Test Workflow"):
                with st.spinner("Testing workflow..."):
                    time.sleep(2)  # Simulate test
                st.success("Workflow test completed successfully!")


def create_advanced_chat_interface():
    """Create an advanced chat interface with enhanced features"""
    
    st.subheader("üí¨ Advanced Legal Research Chat")
    
    # Chat configuration sidebar
    with st.sidebar:
        st.subheader("üõ†Ô∏è Chat Settings")
        
        chat_mode = st.selectbox(
            "Chat Mode",
            ["Standard", "Expert", "Educational", "Quick Query"],
            help="Choose your interaction style"
        )
        
        include_sources = st.checkbox("Include Sources", value=True)
        include_citations = st.checkbox("Extract Citations", value=True)
        confidence_threshold = st.slider("Confidence Threshold", 0.1, 1.0, 0.7)
        
        st.markdown("---")
        
        # Quick actions
        st.subheader("‚ö° Quick Actions")
        if st.button("üìÑ Generate Case Brief"):
            st.session_state.quick_action = "case_brief"
        if st.button("üîç Find Precedents"):
            st.session_state.quick_action = "precedents"
        if st.button("üìä Legal Analysis"):
            st.session_state.quick_action = "analysis"
    
    # Chat interface with enhanced features
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    if "chat_session_id" not in st.session_state:
        st.session_state.chat_session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Display chat history with enhanced formatting
    for i, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"]):
            if message["role"] == "assistant":
                # Enhanced assistant response display
                if isinstance(message["content"], dict):
                    display_enhanced_response(message["content"])
                else:
                    st.markdown(message["content"])
                
                # Add reaction buttons
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    if st.button("üëç", key=f"like_{i}"):
                        log_feedback("like", i)
                with col2:
                    if st.button("üëé", key=f"dislike_{i}"):
                        log_feedback("dislike", i)
                with col3:
                    if st.button("üìã", key=f"copy_{i}"):
                        st.success("Response copied!")
                with col4:
                    if st.button("üîÑ", key=f"regenerate_{i}"):
                        st.info("Regenerating response...")
            else:
                st.markdown(message["content"])
    
    # Enhanced input with suggestions
    suggested_queries = [
        "What are the recent amendments to the Commercial Courts Act?",
        "Explain the procedure for summary judgment in commercial disputes",
        "Find cases related to Section 138 of the Negotiable Instruments Act",
        "What is the limitation period for commercial contracts?"
    ]
    
    with st.expander("üí° Suggested Queries"):
        for query in suggested_queries:
            if st.button(query, key=f"suggest_{query[:20]}"):
                st.session_state.suggested_query = query
    
    # Chat input with enhanced features
    if prompt := st.chat_input("Ask your legal research question..."):
        handle_chat_input(prompt, chat_mode, include_sources, include_citations, confidence_threshold)


def display_enhanced_response(response_data: Dict[str, Any]):
    """Display enhanced response with rich formatting"""
    
    # Main response
    st.markdown("### üìã Analysis Results")
    st.markdown(response_data.get("response", "No response generated"))
    
    # Confidence indicator
    confidence = response_data.get("confidence_score", 0.0)
    confidence_color = "üü¢" if confidence > 0.7 else "üü°" if confidence > 0.4 else "üî¥"
    
    with st.expander("üìä Response Quality", expanded=False):
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Confidence", f"{confidence:.1%}", help="How confident is the AI in this response")
        with col2:
            steps = len(response_data.get("completed_steps", []))
            st.metric("Analysis Steps", steps, help="Number of analysis steps completed")
        with col3:
            citations = len(response_data.get("citations", []))
            st.metric("Citations Found", citations, help="Number of legal citations extracted")
    
    # Additional sections
    tabs = st.tabs(["üîç Sources", "üìö Citations", "‚öñÔ∏è Precedents", "üí° Recommendations"])
    
    with tabs[0]:
        if response_data.get("sources"):
            for source in response_data["sources"]:
                st.write(f"üìÑ {source}")
        else:
            st.info("No specific sources available")
    
    with tabs[1]:
        citations = response_data.get("citations", [])
        if citations:
            for citation in citations:
                st.write(f"‚Ä¢ {citation}")
        else:
            st.info("No citations extracted")
    
    with tabs[2]:
        if response_data.get("precedent_analysis"):
            st.markdown(response_data["precedent_analysis"])
        else:
            st.info("No precedent analysis available")
    
    with tabs[3]:
        recommendations = response_data.get("recommendations", [])
        if recommendations:
            for rec in recommendations:
                st.write(f"üí° {rec}")
        else:
            st.info("No specific recommendations")


def handle_chat_input(prompt: str, mode: str, include_sources: bool, include_citations: bool, threshold: float):
    """Handle chat input with enhanced processing"""
    
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Process with enhanced features based on mode
    with st.chat_message("assistant"):
        with st.spinner("ü§î Analyzing your legal question..."):
            
            # Enhanced processing based on mode
            if mode == "Expert":
                response = process_expert_query(prompt, include_sources, include_citations, threshold)
            elif mode == "Educational":
                response = process_educational_query(prompt)
            elif mode == "Quick Query":
                response = process_quick_query(prompt)
            else:
                response = process_standard_query(prompt, include_sources, include_citations, threshold)
            
            display_enhanced_response(response)
            st.session_state.messages.append({"role": "assistant", "content": response})


def process_expert_query(prompt: str, include_sources: bool, include_citations: bool, threshold: float) -> Dict[str, Any]:
    """Process query in expert mode with comprehensive analysis"""
    
    try:
        integration = LangGraphRAGIntegration()
        result = integration.enhanced_query_processing(
            query=prompt,
            use_langgraph=True,
            documents=None
        )
        
        # Add expert-specific enhancements
        result["analysis_depth"] = "Expert"
        result["expert_insights"] = [
            "Consider potential procedural challenges",
            "Review applicable precedents carefully",
            "Verify citation accuracy",
            "Check for recent amendments"
        ]
        
        return result
        
    except Exception as e:
        return {
            "response": f"Expert analysis failed: {str(e)}",
            "confidence_score": 0.0,
            "error": str(e)
        }


def process_educational_query(prompt: str) -> Dict[str, Any]:
    """Process query in educational mode with learning focus"""
    
    return {
        "response": f"Educational explanation for: {prompt}\n\nThis is a learning-focused response that would explain legal concepts step by step.",
        "confidence_score": 0.8,
        "educational_notes": [
            "Key concepts explained simply",
            "Historical context provided",
            "Related topics suggested",
            "Practice examples included"
        ],
        "learning_resources": [
            "Legal textbooks",
            "Case law databases", 
            "Online legal education"
        ]
    }


def process_quick_query(prompt: str) -> Dict[str, Any]:
    """Process quick query with fast response"""
    
    return {
        "response": f"Quick answer for: {prompt}\n\nThis is a fast, concise response focusing on the most relevant information.",
        "confidence_score": 0.6,
        "quick_facts": [
            "Key point 1",
            "Key point 2", 
            "Key point 3"
        ],
        "for_detailed_analysis": "Switch to Expert mode for comprehensive analysis"
    }


def process_standard_query(prompt: str, include_sources: bool, include_citations: bool, threshold: float) -> Dict[str, Any]:
    """Process standard query with balanced analysis"""
    
    try:
        integration = LangGraphRAGIntegration()
        result = integration.enhanced_query_processing(
            query=prompt,
            use_langgraph=True,
            documents=None
        )
        
        return result
        
    except Exception as e:
        return {
            "response": f"Standard analysis completed with some limitations: {str(e)}",
            "confidence_score": 0.5,
            "error": str(e)
        }


# Utility functions
def check_lm_studio_status() -> bool:
    """Check if LM Studio is accessible"""
    try:
        from langchain_openai import ChatOpenAI
        llm = ChatOpenAI(
            model="local-model",
            openai_api_base="http://127.0.0.1:1234/v1",
            openai_api_key="lm-studio",
            max_tokens=10
        )
        llm.invoke("test")
        return True
    except:
        return False


def check_workflow_status() -> bool:
    """Check if LangGraph workflow is working"""
    try:
        workflow = get_legal_workflow()
        workflow.get_workflow_status()
        return True
    except:
        return False


def generate_analytics_data() -> Dict[str, Any]:
    """Generate sample analytics data"""
    
    return {
        'daily_queries': np.random.randint(100, 500),
        'query_growth': np.random.randint(5, 25),
        'active_users': np.random.randint(10, 50),
        'user_growth': np.random.randint(1, 10),
        'avg_response_time': np.random.uniform(8, 15),
        'response_improvement': np.random.uniform(0.5, 2.5),
        'success_rate': np.random.uniform(88, 98),
        'success_improvement': np.random.uniform(1, 5)
    }


def log_feedback(feedback_type: str, message_index: int):
    """Log user feedback"""
    
    if "feedback_log" not in st.session_state:
        st.session_state.feedback_log = []
    
    st.session_state.feedback_log.append({
        'timestamp': datetime.now(),
        'type': feedback_type,
        'message_index': message_index
    })


# Export main functions
__all__ = [
    'create_modern_header',
    'create_analytics_dashboard', 
    'create_interactive_workflow_builder',
    'create_advanced_chat_interface'
]