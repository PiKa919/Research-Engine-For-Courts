{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline for Indian Legal Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community pypdf chromadb jupyter langchain-google-genai langchain-chroma -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter your Google API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "def ingest_data():\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        os.makedirs(DATA_PATH)\n",
    "        print(f\"Created directory: {DATA_PATH} Awaiting PDF files...\")\n",
    "        return\n",
    "\n",
    "    loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"No documents found in the 'data' directory.\")\n",
    "        return\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"Splitting {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=CHROMA_PATH)\n",
    "    \n",
    "    print(f\"Successfully ingested data and created a vector store at: {CHROMA_PATH}\" )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ingest_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running the Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG Chain with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document outlines rules for legal pleadings, including how to handle documents,  state the effect of documents, and allege things like malice or notice.  It also addresses the organization and presentation of documents in legal proceedings.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def create_rag_chain():\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vectorstore = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
    "\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer the question. \"\n",
    "        \"If you don't know the answer, just say that you don't know. \"\n",
    "        \"Use three sentences maximum and keep the answer concise.\"\n",
    "        \"\\n\\n{context}\"\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", system_prompt), (\"human\", \"{input}\")])\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "    return rag_chain\n",
    "\n",
    "rag_chain = create_rag_chain()\n",
    "\n",
    "query = \"What is the main point of the document?\"\n",
    "response = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
